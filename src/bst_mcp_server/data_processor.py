from datetime import datetime, timedelta
import json
import logging
import os
from bst_mcp_server.config_util import load_field_mapping
from bst_mcp_server.holiday_util import HolidayUtil

# é…ç½®æ—¥å¿—
log_dir = "log"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "data_processor.log")),
        logging.StreamHandler(),
    ],
)
logger = logging.getLogger(__name__)

CONFIG_FILE = "field_mapping.json"


# def load_field_mapping():
#     if not os.path.exists(CONFIG_FILE):
#         print(f"æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ {CONFIG_FILE}")
#         exit()
#     with open(CONFIG_FILE, encoding="utf-8") as f:
#         return json.load(f)


def parse_date(date_str):
    logger.debug(f"è§£ææ—¥æœŸå­—ç¬¦ä¸²: {date_str}")
    try:
        result = datetime.strptime(date_str, "%Y-%m-%d")
        return result
    except ValueError as e:
        logger.error(f"æ—¥æœŸæ ¼å¼æ— æ•ˆ: {date_str}")
        raise


def date_diff(start_str, end_str):
    """
    è®¡ç®—ä¸¤ä¸ªæ—¥æœŸä¹‹é—´çš„æœ‰æ•ˆå·¥ä½œæ—¥å¤©æ•°ï¼ˆæ‰£é™¤å‘¨æœ«å’Œæ³•å®šèŠ‚å‡æ—¥ï¼‰

    å‚æ•°ï¼š
    - start_str: å¼€å§‹æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    - end_str: ç»“æŸæ—¥æœŸå­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰

    è¿”å›ï¼š
    - æœ‰æ•ˆå·¥ä½œæ—¥å¤©æ•°
    """
    logger.info(f"è®¡ç®—æ—¥æœŸå·®: {start_str} åˆ° {end_str}")
    # æ—¥æœŸæ ¼å¼éªŒè¯
    try:
        start = parse_date(start_str)
        end = parse_date(end_str)
    except ValueError:
        error_msg = "æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œåº”ä¸ºYYYY-MM-DD"
        logger.error(error_msg)
        raise ValueError(error_msg)

    # ç»“æŸæ—¶é—´æ£€æŸ¥
    if start > end:
        error_msg = "ç»“æŸæ—¥æœŸä¸èƒ½æ—©äºå¼€å§‹æ—¥æœŸ"
        logger.error(error_msg)
        raise ValueError(error_msg)

    holiday_util = HolidayUtil()
    day_count = 0
    current = start

    while current <= end:
        # åˆ¤æ–­æ˜¯å¦ä¸ºå·¥ä½œæ—¥ï¼ˆå‘¨ä¸€åˆ°å‘¨äº”ï¼‰ä¸”ä¸æ˜¯èŠ‚å‡æ—¥
        if current.weekday() < 5 and not holiday_util.is_holiday(
            current.strftime("%Y-%m-%d")
        ):
            day_count += 1
        current += timedelta(days=1)

    logger.info(f"æœ‰æ•ˆå·¥ä½œæ—¥å¤©æ•°: {day_count}")
    return day_count


def get_predecessor_list(issue):
    """
    ä» issue çš„ issuelinks ä¸­æå–æ‰€æœ‰ç¬¦åˆè§„åˆ™çš„å‰ç½®ä»»åŠ¡ keyï¼Œç»„æˆ list
    ä¼˜å…ˆçº§é¡ºåº:
        'has to be done after' > 'is blocked by' > 'is child of'
    è¿”å›: list of keys
    """
    logger.debug("æå–å‰ç½®ä»»åŠ¡åˆ—è¡¨")
    # å®šä¹‰ä¼˜å…ˆçº§é¡ºåº
    priority_order = {"has to be done after": 0, "is blocked by": 1, "is child of": 2}

    # å­˜å‚¨ç¬¦åˆæ¡ä»¶çš„ linksï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ†ç»„
    grouped_links = {
        0: [],  # has to be done after
        1: [],  # is blocked by
        2: [],  # is child of
    }

    # éå†æ‰€æœ‰é“¾æ¥
    for link in issue.get("fields", {}).get("issuelinks", []):
        inward_issue = link.get("inwardIssue")
        link_type = link.get("type", {})
        inward_type = link_type.get("inward")

        if inward_issue and inward_type in priority_order:
            predecessor_key = inward_issue.get("key")
            if predecessor_key:
                priority_level = priority_order[inward_type]
                grouped_links[priority_level].append(predecessor_key)
                logger.debug(f"æ‰¾åˆ°å‰ç½®ä»»åŠ¡: {predecessor_key}, ç±»å‹: {inward_type}")

    # æ‰¾å‡ºæœ€é«˜ä¼˜å…ˆçº§å¹¶è¿”å›å¯¹åº”çš„æ‰€æœ‰ key
    # æ‰¾å‡ºæœ€é«˜ä¼˜å…ˆçº§ä¸”éç©ºçš„åˆ†ç»„
    highest_priority_group = []
    for level in sorted(grouped_links.keys()):
        if grouped_links[level]:
            highest_priority_group = grouped_links[level].copy()
            logger.debug(f"ä½¿ç”¨ä¼˜å…ˆçº§ {level} çš„å‰ç½®ä»»åŠ¡: {highest_priority_group}")
            break

    # è·å– parent çš„ keyï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    # parent_key = None
    # parent = issue.get("fields", {}).get("parent")
    # if parent and isinstance(parent, dict):
    #     parent_key = parent.get("key")

    # å¦‚æœæœ‰ parentï¼ŒåŠ å…¥åˆ—è¡¨ï¼Œå¹¶å»é‡
    # if parent_key:
    #     highest_priority_group.append(parent_key)

    # å»é‡å¹¶è¿”å›
    result = list(dict.fromkeys(highest_priority_group))
    logger.debug(f"æœ€ç»ˆå‰ç½®ä»»åŠ¡åˆ—è¡¨: {result}")
    return result


def get_prelink_list(issue):
    """
    ä» issue çš„ issuelinks ä¸­æå–æ‰€æœ‰ç¬¦åˆè§„åˆ™çš„å‰ç½®ä»»åŠ¡ keyï¼Œç»„æˆ list
    ä¼˜å…ˆçº§é¡ºåº:
        'has to be done after' > 'is blocked by' > 'is child of'
    è¿”å›: list of keys
    """
    logger.debug("æå–å‰ç½®é“¾æ¥åˆ—è¡¨")
    # å®šä¹‰ä¼˜å…ˆçº§é¡ºåº
    prelink_types = ["has to be done after", "is blocked by", "is child of"]

    # ç”¨äºå­˜å‚¨ç»“æœçš„å­—å…¸ï¼ŒæŒ‰ç±»å‹åˆ†ç±»
    result_map = {link_type: [] for link_type in prelink_types}

    # éå†æ‰€æœ‰é“¾æ¥
    for link in issue.get("fields", {}).get("issuelinks", []):
        inward_issue = link.get("inwardIssue")
        link_type = link.get("type", {})
        inward_type = link_type.get("inward")

        # å¦‚æœé“¾æ¥ç±»å‹åœ¨æˆ‘ä»¬å…³æ³¨çš„ä¼˜å…ˆçº§ç±»å‹ä¸­
        if inward_type in prelink_types and inward_issue:
            predecessor_key = inward_issue.get("key")
            if predecessor_key:
                result_map[inward_type].append(predecessor_key)
                logger.debug(f"æ‰¾åˆ°å‰ç½®é“¾æ¥: {predecessor_key}, ç±»å‹: {inward_type}")

    # æŒ‰ä¼˜å…ˆçº§é¡ºåºåˆå¹¶ç»“æœ
    ordered_result = []
    for link_type in prelink_types:
        ordered_result.extend(result_map[link_type])

    logger.debug(f"å‰ç½®é“¾æ¥åˆ—è¡¨: {ordered_result}")
    return result_map


def extract_task_info(data, field_mapping):
    logger.info("å¼€å§‹æå–ä»»åŠ¡ä¿¡æ¯")
    customfield_id_map = {}
    for field_id, field_name in data.get("names", {}).items():
        customfield_id_map[field_name] = field_id

    tasks = []
    issues = data.get("issues", [])
    logger.info(f"å…±å¤„ç† {len(issues)} ä¸ªé—®é¢˜")

    for issue in issues:
        key = issue["key"]
        logger.debug(f"å¤„ç†é—®é¢˜: {key}")
        fields = issue.get("fields", {})

        task = {
            "key": key,
            "issueId": issue["id"],
            # 'plan_start': None,
            # 'plan_end': None,
            # 'actual_start': None,
            # 'actual_end': None,
            # 'predecessor': None
        }

        for field_key, config in field_mapping.items():
            field_type = config.get("type")
            path = config.get("path")

            if field_type == "custom":
                display = config.get("display")
                field_id = customfield_id_map.get(display)
                if field_id:
                    value = fields.get(field_id)
                    if value is not None:
                        if path:
                            value = get_value(value, path)
                            if value is not None:
                                task[field_key] = value
                                logger.debug(f"è®¾ç½®å­—æ®µ {field_key}: {value}")
                        else:
                            task[field_key] = value
                            logger.debug(f"è®¾ç½®å­—æ®µ {field_key}: {value}")

            elif field_type == "system":
                parts = path.split(".")
                value = fields
                value = get_value(value, path)
                if value is not None:
                    task[field_key] = value
                    logger.debug(f"è®¾ç½®å­—æ®µ {field_key}: {value}")

        # âœ… æ–°å¢ï¼šè®¾ç½®å‰ç½®ä»»åŠ¡åˆ—è¡¨
        predecessor_list = get_predecessor_list(issue)
        task["predecessors"] = predecessor_list
        task["prelinks"] = get_prelink_list(issue)
        tasks.append(task)

    logger.info(f"ä»»åŠ¡ä¿¡æ¯æå–å®Œæˆï¼Œå…±æå– {len(tasks)} ä¸ªä»»åŠ¡")
    return tasks


def get_value(value, path):
    logger.debug(f"è·å–å€¼: path={path}")
    parts = path.split(".")
    for part in parts:
        if isinstance(value, list) and len(value) > 0:
            found_items = []
            for item in value:
                if isinstance(item, dict) and part in item:
                    found_items.append(item[part])

            if len(found_items) > 0:
                # å¦‚æœæ˜¯æœ€åä¸€çº§è·¯å¾„ï¼Œè¿”å›æ•´ä¸ªåˆ—è¡¨
                # å¦åˆ™ç»§ç»­å¤„ç†ä¸‹ä¸€çº§ï¼ˆæ¯ä¸ªå…ƒç´ ç»§ç»­å‘ä¸‹éå†ï¼‰
                value = found_items
            else:
                value = None
                break
        elif isinstance(value, dict) and part in value:
            value = value[part]
        else:
            value = None
            break
    logger.debug(f"è·å–åˆ°å€¼: {value}")
    return value


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    log_dir = "log"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.FileHandler(os.path.join(log_dir, "data_processor_main.log")),
            logging.StreamHandler(),
        ],
    )
    logger = logging.getLogger(__name__)

    field_mapping = load_field_mapping()
    with open("WORK.log", encoding="utf-8") as f:
        raw_data = json.load(f)
    tasks = extract_task_info(raw_data, field_mapping)
    # ğŸ” é‡å®šå‘ stdout åˆ°æ—¥å¿—æ–‡ä»¶
    logger.info(json.dumps(tasks, ensure_ascii=False, indent=2))
